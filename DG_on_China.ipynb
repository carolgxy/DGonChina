{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81a41fbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DG_China\n",
    "This project is based on the Deep Gavity model, which is based on 20 years of national inter-city foot traffic of Gaode Map as a sample of mobility, with a view to predicting the changes in the attractiveness of cities in the urban agglomerations to the population from 2025."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a5842a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed982d02-977e-425d-b0bf-d183f8ff354f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,torch,time,random,re,os\n",
    "from sklearn import linear_model\n",
    "\n",
    "from DGModel import FlowDataset, instantiate_model\n",
    "\n",
    "db_dir = './data/'                #database_dir\n",
    "\n",
    "osm_path = db_dir + 'data_osm/'\n",
    "flows_path = db_dir + 'data_flows/'\n",
    "population_path = db_dir + 'data_population/'\n",
    "prediction_path = db_dir + 'data_prediction/'\n",
    "\n",
    "pt_file_path     = 'result/model_china.pt'\n",
    "result_file_path = 'result/result_25.csv'\n",
    " \n",
    "city_info = pd.read_csv(db_dir + 'city_info.csv') \n",
    "forecast_city_code_list = city_info.dropna(axis=0,how='any')['city_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0c2ce4-0f2e-4887-bc7c-a15416756fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "device = 'gpu'\n",
    "seed = 1234\n",
    "lr = 5e-6\n",
    "momentum = 0.9\n",
    "epochs = 60\n",
    "log_interval = 1\n",
    "\n",
    "cuda = 1\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch_device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b2864e0-653b-4f45-9135-16837f3caecb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "225922c2-eaeb-4e6f-8ad8-2e9d53ef7a80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-1. Function preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2381ec0a-77fa-4a51-8797-c8eae95d5768",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    training_acc = 0.0\n",
    "\n",
    "    for batch_idx, data_temp in enumerate(train_loader):\n",
    "        b_data = data_temp[0]\n",
    "        b_target = data_temp[1]\n",
    "        ids = data_temp[2]\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        for data, target in zip(b_data, b_target):\n",
    "\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = model.forward(data)\n",
    "            loss += model.loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            if batch_idx * len(b_data) == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} \\tLoss: {:.6f}'.format(epoch, batch_idx * len(b_data), len(train_loader),\n",
    "                                                                     running_loss / len(train_dataset)))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.\n",
    "        test_accuracy = 0.\n",
    "        n_origins = 0\n",
    "        for batch_idx, data_temp in enumerate(test_loader):\n",
    "            b_data = data_temp[0]\n",
    "            b_target = data_temp[1]\n",
    "            ids = data_temp[2]\n",
    "\n",
    "            for data, target in zip(b_data, b_target):\n",
    "                if cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output = model.forward(data)\n",
    "                test_loss += model.loss(output, target).item()\n",
    "\n",
    "                cpc = model.get_cpc(data, target)\n",
    "                test_accuracy += cpc\n",
    "                n_origins += 1\n",
    "\n",
    "        test_loss /= n_origins\n",
    "        test_accuracy /= n_origins\n",
    "        \n",
    "        return test_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfc0b54e-9037-488d-b1ef-8b0b9e57e2b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 2-2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a8f5f6-f98e-44d2-941f-f768da98c575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[df]features: 9 features, latitude and longitude of each city in the forecast year are needed\n",
    "features_name = ['transport','traffic','roads','railway','poi','places','landuse','building','population']\n",
    "\n",
    "features = pd.read_csv(osm_path + 'osm_20_china.csv', engine='python', encoding='utf-8')\n",
    "population_csv = pd.read_csv(population_path + 'population_20_china.csv', engine='python')\n",
    "\n",
    "features = pd.merge(features, population_csv.loc[:,['city_code','population']], how='left', on='city_code', copy=False)\n",
    "features = features.set_index('city_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7febee4c-617e-4ac1-a8f7-e74745f15524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dict]oa2features    \n",
    "oa2features = dict()\n",
    "for oa in city_info['city_code']:\n",
    "    oa_features = list(features.loc[oa,features_name].values)\n",
    "    oa_features = list(np.log(oa_features))\n",
    "    oa2features[oa] = oa_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3095c031-8d68-4610-a0c6-c510f06064b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dict]o2d2flow  \n",
    "od = pd.read_csv(flows_path + 'flows_20.csv')\n",
    "od = od.groupby(['from_city_code','to_city_code'])['real_value'].sum()\n",
    "o2d2flow = {}\n",
    "for (o,d),f in od.items():      #Convert tabel to dictionary\n",
    "    try:\n",
    "        d2f = o2d2flow[o]\n",
    "        d2f[d] = f\n",
    "    except KeyError:\n",
    "        o2d2flow[o] = {d: f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad70e77-497c-46b5-bb72-678d0b868917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dict]tileid2oa2features2vals\n",
    "tileid2oa2features2vals = {}\n",
    "for _,(tile_num,city_num) in city_info.loc[:,['tile_num','city_code']].iterrows():\n",
    "    city_features = features.loc[city_num, features_name].to_dict()\n",
    "    try:\n",
    "        reg = tileid2oa2features2vals[tile_num]\n",
    "        reg[city_num] = city_features\n",
    "    except KeyError:\n",
    "        tileid2oa2features2vals[tile_num] = {city_num:city_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aba5ae3-fdd2-4e47-8e35-6aab2d883993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dict]oa2centroid \n",
    "oa2centroid = {key: [float(features['latitude'][key]), float(features['longitude'][key])]  for key in features.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8efcc37e-3df1-4833-b1e5-18ed1726cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [list]train_data,test_data: selected the tile_id for train and test\n",
    "\n",
    "random_index = np.load(db_dir + 'random_index.npy')\n",
    "half_index = int(len(random_index)/2)\n",
    "\n",
    "train_data_index = random_index[0:half_index]     #tileid for train\n",
    "test_data_index = random_index[half_index+1:]     #tileid for test\n",
    "tileid_keys = list(tileid2oa2features2vals.keys())\n",
    "train_data = [m for t in train_data_index for m in list(tileid2oa2features2vals[tileid_keys[t]].keys())]\n",
    "test_data = [m for t in test_data_index for m in list(tileid2oa2features2vals[tileid_keys[t]].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8bfe99-98e0-4650-9043-59d8ef972408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\\\n",
    "                      'o2d2flow': o2d2flow,\\\n",
    "                      'oa2features': oa2features,\\\n",
    "                      'oa2centroid': oa2centroid,\\\n",
    "                      'dim_dests': 512,\\\n",
    "                      'frac_true_dest': 0.0}\n",
    "test_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\\\n",
    "                     'o2d2flow': o2d2flow,\\\n",
    "                     'oa2features': oa2features,\\\n",
    "                     'oa2centroid': oa2centroid,\\\n",
    "                     'dim_dests': int(1e9),\\\n",
    "                     'frac_true_dest': 0.0}\n",
    "\n",
    "train_dataset = FlowDataset(train_data, **train_dataset_args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = FlowDataset(test_data, **test_dataset_args)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size)   #让数据做核酸排好队\n",
    "\n",
    "dim_input = len(train_dataset.get_features(train_data[0], train_data[0]))          #equal 19"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1a235f6-44ae-4535-ac7a-b8dfbe76ee51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-2. Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58930857-95ee-4df8-af50-846ad418f813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [163/164 \tLoss: 124.872150\n",
      "0.5673759373961054\n",
      "Saving model to result/model_china.pt ...\n",
      "Train Epoch: 2 [163/164 \tLoss: 113.784211\n",
      "0.6257968718872133\n",
      "Saving model to result/model_china.pt ...\n",
      "Total training time: 7.279669761657715 seconds\n",
      "Computing the CPC on test set, loc2cpc_numerator ...\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_model(oa2centroid, oa2features, dim_input, device=torch_device)\n",
    "if device.find(\"gpu\") != -1:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "t0 = time.time()\n",
    "test_accuracy = test(0)\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # set new random seeds\n",
    "    torch.manual_seed(seed + epoch)\n",
    "    np.random.seed(seed + epoch)\n",
    "    random.seed(seed + epoch)\n",
    "\n",
    "    train(epoch)\n",
    "    test_accuracy = test(epoch)\n",
    "    print(test_accuracy)\n",
    "\n",
    "    if best_test_accuracy < test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "\n",
    "        print('Saving model to {} ...'.format(pt_file_path))\n",
    "        torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, pt_file_path)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Total training time: %s seconds\" % (t1 - t0))\n",
    "print('Computing the CPC on test set, loc2cpc_numerator ...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df6d7a6e-a97a-496f-98cc-ec1cfe7e83cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406ff260-7cf4-4ff9-8481-3c6aa91d4e4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-1. Function preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d60b12e-36d1-4864-98ed-1a55d8772a84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \n",
    "    result = list()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_idx, data_temp in enumerate(generate_loader):\n",
    "            b_data = data_temp[0]\n",
    "            from_city_code = data_temp[2][0].tolist()[0]\n",
    "            to_city_list = data_temp[3][0].tolist()[0]\n",
    "\n",
    "            for data in b_data:\n",
    "                if cuda:\n",
    "                    data = data.cuda()\n",
    "\n",
    "                output = model.forward(data)\n",
    "                soft = torch.nn.functional.softmax(output,dim=1).reshape(1,-1)\n",
    "                soft = soft[0].data.tolist()\n",
    "                \n",
    "                for index in range(len(to_city_list)):\n",
    "                    result.append([int(from_city_code), int(to_city_list[index]), soft[index]])\n",
    "        \n",
    "        return pd.DataFrame(result, columns=['from_city_code', 'to_city_code', 'forecast'],dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d66ec35-a489-45a6-854c-1a5f9e3ee83c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-2. Data preparation for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3eaedff-1df6-4f93-b90d-906188e0d4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [df]features: 9 features, latitude and longitude of each city in the forecast year are needed\n",
    "features_name = ['transport','traffic','roads','railway','poi','places','landuse','building','population']\n",
    "\n",
    "features = pd.read_csv(prediction_path + '/osm_25.csv', engine='python', encoding='utf-8')\n",
    "population_csv = pd.read_csv(population_path + 'population_25_agglomerate.csv', engine='python',encoding='utf-8')\n",
    "\n",
    "features = pd.merge(features, population_csv.loc[:,['city_code','population']], how='left', on='city_code', copy=False)\n",
    "features = pd.merge(features, city_info.loc[:,['city_code','longitude','latitude']], how='left', on='city_code', copy=False)\n",
    "features= features.set_index('city_code')\n",
    "\n",
    "city_info_agglomerate = city_info.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d01dfc-a208-4a71-8e2b-ac55dd1e868c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [dict]oa2features\n",
    "oa2features = dict()\n",
    "for oa in city_info_agglomerate['city_code']:\n",
    "    oa_features = list(features.loc[oa,features_name].values)\n",
    "    oa_features = list(np.log(oa_features))\n",
    "    oa2features[oa] = oa_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8edc6aa-527c-4ea4-b13b-fbd710ac55c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [dict]o2d2flow\n",
    "o2d2flow = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f57a729-5c2a-4ce8-830a-0130fb25f2a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [dict]tileid2oa2features2vals\n",
    "tileid2oa2features2vals = {}\n",
    "for _,(tile_num,city_num) in city_info_agglomerate.loc[:,['agglomerate','city_code']].iterrows():\n",
    "    city_features = features.loc[city_num, features_name].to_dict()\n",
    "    try:\n",
    "        reg = tileid2oa2features2vals[tile_num]\n",
    "        reg[city_num] = city_features\n",
    "    except KeyError:\n",
    "        tileid2oa2features2vals[tile_num] = {city_num:city_features}\n",
    "        \n",
    "# [dict]oa2centroid \n",
    "oa2centroid = {key: [float(features['latitude'][key]), float(features['longitude'][key])]  for key in features.index}\n",
    "\n",
    "generate_data = list(features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d7a3688-f9e7-4c00-a760-d198be59dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\\\n",
    "                      'o2d2flow': o2d2flow,\\\n",
    "                      'oa2features': oa2features,\\\n",
    "                      'oa2centroid': oa2centroid,\\\n",
    "                      'dim_dests': 512,\\\n",
    "                      'frac_true_dest': 0.0}\n",
    "\n",
    "generate_dataset = FlowDataset(generate_data, **generate_dataset_args)\n",
    "generate_loader = torch.utils.data.DataLoader(generate_dataset, batch_size=test_batch_size)\n",
    "\n",
    "dim_input = len(generate_dataset.get_features(generate_data[0], generate_data[0])) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c4c35c-ebc4-4f4e-be04-5790a9183054",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71996514-7eaa-45bd-8b02-6f8261081677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_model(oa2centroid, oa2features, dim_input, device=torch_device)\n",
    "if device.find(\"gpu\") != -1:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "checkpoint = torch.load(pt_file_path)                                                #load the trained model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "result = generate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0df02bf4-bd9a-4705-aee8-e4facd6c05c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-4. Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f8094f-1daa-4b4d-83ab-905675b2d9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = pd.merge(result,city_info.astype(str).loc[:,['city_code','longitude','latitude','city']],left_on='from_city_code',right_on='city_code',copy=False)\n",
    "result = result.drop('city_code',axis=1).rename(columns={'longitude':'from_city_longitude','latitude':'from_city_latitude','city':'from_city_name'})\n",
    "result = pd.merge(result,city_info.astype(str).loc[:,['city_code','longitude','latitude','city']],left_on='to_city_code',right_on='city_code',copy=False)\n",
    "result = result.drop('city_code',axis=1).rename(columns={'longitude':'to_city_longitude','latitude':'to_city_latitude','city':'to_city_name'})\n",
    "\n",
    "result.to_csv(result_file_path)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
